
Intro to Stats

Search this site
CONTENTS
R BOOTCAMP
MYDESKTOP
DATA REPOSITORY
CLASS WORKSHEETS
GOOGLE+ COMMUNITY
FEEDBACK
CONTENTS
1 Module 9 - Simple Linear Regression and Correlation
1.1 Overview and Learning Objectives
1.2 Linear Regression
1.3 Example - Oxygen Uptake Efficiency Slope
1.3.1 Assumptions
1.3.2 Example Write-up
1.4 Correlation
1.4.1 Example Write-up

Module 9 - Simple Linear Regression and Correlation

Overview and Learning Objectives

Statistical investigations often aim to understand the relationship between variables in order to make accurate predictions. This module will cover the use of linear regression models for modelling relationships between two quantitative variables. The learning objectives associated with this module are:

Define the common terms of linear regression and correlation.
Explain the concept behind the least squares method for simple linear regression. 
Interpret simple scatter plots visualising bivariate data. 
Use technology to fit a simple linear regression and perform hypothesis tests of the various model components. 
Use technology to test the various assumptions behind linear regression analysis and identify when assumptions are in doubt. 
Interpret the output of a simple linear regression analysis.
Use technology to compute the Pearson correlation coefficient and perform hypothesis testing.
Interpret the Pearson correlation coefficient and determine when a correlation can be considered statistically significant. 
Linear Regression

Correlation and simple linear regression are used to examine the relationship between two quantitative (discrete or continuous) variables. These methods assume that a predictor variable, x, provides information about some dependent variable, y. In practice, an investigation may have many predictors, xi, but to introduce the basic concepts we will consider the simplest case of one predictor. We write a simple linear regression equation as:

 
where y is the dependent variable, a is the constant/intercept, ß is the slope, x is the predictor and e is the random error/residuals. Linear regression is a parametric method because e are assumed to be normally distributed with µ = 0 and s. Linear regression also assumes that  the variance of e is constant and unchanging across the range of the predictor variable, x. For linear regression, we need to check these assumptions carefully, but we can only do so after the model has been fit.

Linear regression also assumes that the relationship between the predictor and dependent variable is explained by a linear, or straight line, relationship. If the relationship is not linear, linear regression should not be used. The following scatter plots show examples of linear and non-linear bivariate relationships.


Fitting a linear regression line to sample data is done using a method known as ordinary least squares (OLS). The idea behind this method is to minimise the sum of squared distances, S, for each (xi, yi) bivariate data point from a fitted regression line. The sum of squares is written as:


where di refers to the (xi, yi) pairs' deviation from the regression line. The regression line that minimises this value is known as the line of best fit. Check out this excellent interactive visualisation site by Victor Powell and Lewis Lehe. The site is embedded in the following frame (if it does not appear, ensure you configure your browser to display the page, otherwise you can access the site directly here). Work through the steps of the visualisations to get a better grasp of OLS.


OLS seeks to minimise the sum of squares, S, by determining a line of best fit. 

Finding the line of best fit is a mathematical exercise. The formulas will be presented here for completeness, but in practice this is better left to R. To find the line of best fit, we calculate the raw sum for x:


and the raw sum for y:



Next, the raw sum of squares for x:


Followed by the raw sum of squares for y:


Then we calculate the raw sum of the cross product...


We correct the raw sum values by adjusting them to the squared deviation from the mean. First for x:


Then for y:


Finally we calculate the corrected sum of the cross products:


The slope of the line of best fit is computed as:



with an intercept equal to:



As you can tell, this isn't a lot of fun by hand, especially when the sample size gets large. Fortunately, we have computers that can do all the hard work of finding the line of best fit in order to estimate the regression equation. This means we can use our precious time to focus on understanding the concepts and testing hypotheses. We will look at the following example to help explain the meaning behind the regression concepts.

Example - Oxygen Uptake Efficiency Slope

Maximal oxygen consumption (VO2 max) is a measure of aerobic fitness. However, measuring VO2 max requires a subject to fully exert their aerobic system. This can be very impractical (e.g. time consuming) and dangerous in certain populations (e.g. the elderly or unwell). Researchers are interested to know if the oxygen uptake efficiency slope (OUES), an indicator of cardiopulmonary reserve, can be used as a sub-maximal predictor of VO2 max. The advantage of OUES is that it can be measured much earlier in VO2 max testing before the subject reaches aerobic exertion. This would make it a more practical and safe measure of aerobic fitness. The OUES.csv dataset contains the OUES measured at three minutes into a VO2 max test and final VO2 readings of 50 healthy adults. The dataset can be downloaded from the Data Repository. 

The following linear regression spreadsheet shows how to work through the formulas described in the previous section. However, this is better left to R.

LINEAR REGRESSION WORKSHEET - INTRO TO STATS

In the following sections, we will work through performing a simple linear regression using R. Along the way, we will look at checking assumptions, interpreting important output and testing the statistical hypotheses of a linear regression model. The first step for all linear regression analyses is to plot the relationship between your x and y variables, OUES and VO2 max, to determine if linear regression is suitable. This is best achieved using a simple scatter plot.

> library(mosaic)
> xyplot(VO2_Max ~ OUES_3, data = OUES, ylab ="VO2 Max",xlab="OUES - 3 Minutes")

The data exhibits a positive linear trend. A positive linear relationship occurs when as the predictor variable increases in value,  so too do the values for the dependent variable. In this situation, higher OUES values are associated with higher VO2 max. This makes scientific sense. However, if VO2 max had decreased with increasing values for OUES, the relationship would be negative. 

As the data exhibit signs of a positive linear relationship, we can proceed with fitting the linear regression model using R. We will work through the important code and output. Let's run the regression. 

> ouesvo2maxmodel <- lm(VO2_Max ~ OUES_3, data = OUES)
> msummary(ouesvo2maxmodel)
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 2.107e+03  1.928e+02   10.93 1.28e-14 ***
OUES_3      6.085e-01  5.969e-02   10.19 1.35e-13 ***

Residual standard error: 567.2 on 48 degrees of freedom
Multiple R-squared:  0.684, Adjusted R-squared:  0.6775 
F-statistic: 103.9 on 1 and 48 DF,  p-value: 1.345e-13
The first part of the code assigns the linear regression model lm() to an object named ouesvo2maxmodel. This will allow you to call this object to summarise the model and later test assumptions. The msummary() function prints out a summary of the important output required to interpret the linear regression model. 

The model summary reports the R2 statistic. To calculate R2 look back to the linear regression worksheet and calculate the following formula.

 

The R2 value can range from 0 - 1. R2  reflects the proportion of variability in the dependent variable that can be explained by a linear relationship with the predictor variable. Therefore, OUES measured at 3 minutes, explained 68.4% of the variability in final VO2 max readings. The R2 is a measure of goodness of fit for linear regression. The better the line fits the data (i.e. the closer the data points sit on the line) the higher R2 will be. If there is no linear relationship between the predictor and dependent variable, R2 = 0 or close to it. You will also notice an Adjusted R2 value. R2 tends to overestimate the population R2 . The adjusted R2  takes this overestimation into account and down-scales it. Which do you report? It does not really matter, just as long as you're clear on which one you use.

The model summary also reports an F statistic which is used to test the overall regression model. The F-test for the linear regression has the following statistical hypotheses:

H0: The data do not fit the linear regression model

HA: The data fit the linear regression model

This test is more useful when you deal with multiple predictors, but we will explain it here so you know what it means. Assuming the data do not fit a linear model in the population, the F statistic reported in the summary as F = 103.9, will have a F distribution with df1 = 1 and df2 = n - 2 = 50 - 2 = 48. The F distribution is positively skewed, so to calculate the p-value of the the observed F statistic, we need to find Pr(F1, n-2 > F). This is easy using the qf() function in R.

> pf(103.9,1,48,lower.tail = FALSE)
[1] 1.348976e-13
We confirm the p-value reported in the summary to be p < .001. What does this mean? As p is less than the 0.05 level of significance, we reject H0. There was statistically significant evidence that the data fit a linear regression model. How was the actual F statistic calculated? We can compute it directly from the R2 statistic:



> (0.684/(1-0.684)*(48/1))
[1] 103.8987
We can also use the values back in the worksheet.



Or, we can quickly get the RegSS and ResSS values using the anova() function in R.

> anova(ouesvo2maxmodel)
Analysis of Variance Table

Response: VO2_Max
          Df   Sum Sq  Mean Sq F value    Pr(>F)    
OUES_3     1 33437664 33437664  103.92 1.345e-13 ***
Residuals 48 15444888   321768                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Now we can solve for F:



The next part of the model summary is the all important one. The coefficients table reports the sample estimates for the intercept/constant, a, and slope, b. Let's begin with our interpretation of the intercept or constant.

> coef(summary(ouesvo2maxmodel))
                Estimate   Std. Error  t value     Pr(>|t|)
(Intercept) 2106.8638073 192.80364251 10.92751 1.282351e-14
OUES_3         0.6085115   0.05969288 10.19404 1.345027e-13

The intercept/constant is reported as a = 2106.864. The constant, or intercept, is the average value for y when x = 0. In this example, this value represents the average V02 max score when OUES is equal to 0. Given that an OUES of 0 is impossible (assuming you're alive) the constant typically has no meaningful interpretation. To test the statistical significance of the constant, we set the following statistical hypotheses: 

H0: a = 0 
HA: a ? 0
This hypothesis is tested using a t statistic, reported as t = 10.928, p < .001. The constant is statistically significant at the 0.05 level. This means that there is statistically significant evidence that the constant is not 0. The t statistic for the constant was calculated as:


The term s2y.x is the ResMS value reported in the ANOVA table. Lxx was taken from the regression worksheet calculations. The denominator for the standard error for a is conveniently reported in the R coefficients summary as 192.804. To calculate the two-tailed p-value for the constant, we use the following R formula:

> 2*pt(10.928,50-2,lower.tail=FALSE)
[1] 1.28037e-14
We confirm that p < .001. R can also report a 95% CI for a. This is calculated as:


In R, we use the confint() function:

> confint(ouesvo2maxmodel)
                   2.5 %      97.5 %
(Intercept) 1719.2061023 2494.521512
OUES_3         0.4884909    0.728532
R reports the 95% CI for a to be [1719.206, 2494.522]. H0: a = 0 is clearly not captured by this interval, so was rejected.

The slope of the regression line was reported as b = 0.609. The slope represents the average increase in y following a one unit increase in x. In relation to the example, a one unit increase in OUES was related to an average increase in VO2 max of .609 units. This is a positive change. Had the slope been a negative number, VO2 max would decrease. The hypothesis test of the slope, b, was as follows:

H0: ß = 0 
HA: ß ? 0
The slope was also tested using a t statistic which was reported as t = 10.194, p < .001. t was calculated as:


To calculate the two-tailed p-value for the slope in this example, we compute:

> 2*pt(10.19,50-2,lower.tail=FALSE)
[1] 1.362792e-13
and confirm that p < .001. As p < .05, we reject H0. There was statistically significant evidence that OUES was positively related to VO2 max.

The 95% CI for the slope can be calculated as:


Looking back to the confint() function, R reports the 95% CI for b to be [.488, .729]. This 95% CI does not capture H0, therefore it was rejected. There was a statistically significant positive relationship between OUES measurements taken at 3 minutes and final VO2 max. Finally, a nice plot to summarise the linear relationship:

> xyplot(VO2_Max ~ OUES_3, data = OUES, ylab ="VO2 Max",
    xlab="OUES - 3 Minutes",
    panel=panel.lmbands)

The blue line is the line of best fit for the linear regression. The green bands represent the 95% CI of mean VO2 max readings for the regression line. The pink outer lines are the prediction intervals. The prediction intervals are where 95% of the data will fall assuming the residuals are normally distributed.

Assumptions

Before we report the final regression model, we must validate all the following assumptions for linear regression. 
Independence 
Linearity 
Normality of residuals 
Homoscedasticity 
Independence is checked through the research design. You must ensure that all measurements between participants or observations are independent, for example, you have not included multiple measurements from the same people or knowing the measurements of one person do not share a relationship with other measurements. 

We have already checked and confirmed linearity earlier in the notes. The idea is to rule out non-linear relationships. 

The assumption of normality for linear regression relates to the distribution of the errors or residuals for the model. The residuals are calculated as:


where yi was an observed score in the sample and yi was the predicted score based on the fitted regression model. For example, the predicted score for OUES = 4000 was:


So, for each observed score in the sample, the predicted score was calculated and the residuals recorded. The residuals reflect how far an observed score yi deviates from the y score predicted by the line of best fit. We can run a normal Q-Q plot to check the normality of the residuals:

> qqPlot(ouesvo2maxmodel$residuals, dist="norm")

We check the normal Q-Q plot to determine if there were any gross deviations (e.g obvious S shapes or non-linear trends). The plot above suggests there are no major deviations from normality. It would be safe to assume the residuals are at least approximately normally distributed. 

The final assumption that we need to check, and the most important, is the assumption of homoscedasticity, or constant variance. Homoscedasticity is related to the assumption of homogeneity of variance for the two-sample t-test. We check this assumption by looking at a scatter plot of the predicted values on the x axis and the residuals on the y. As we move across predicted values, the variance in the residuals should remain constant. In the plot below, the variance appears to remain the same. The blue line in the plot is a non-parametric locally weighted scatterplot smoother (LOESS). The line fits to the data. The straighter the line, the safer the assumption of homoscedasticity. If the variance changed across predicted values, we would call the data heteroscedastic. Ordinary least squares linear regression is not appropriate for heteroscedastic data.

> mplot(ouesvo2maxmodel, 1)

You might be wondering what heteroscedasticity looks like. The following figure taken from Osborne and Waters (2002) provides a guide. As you can see, the OUES regression model was pretty safe.

Example Write-up

A linear regression model was fitted to predict the dependent variable, VO2 max, using measures of OUES taken at 3 minutes as a single predictor. Prior to fitting the regression, a scatterplot assessing the bivariate relationship between VO2 max and OUES 3 minutes was inspected. The scatterplot demonstrated evidence of a positive linear relationship. Other non-linear trends were ruled out. The overall regression model was statistically significant, F(1, 48) = 103.92, p < .001, and explained 68.4% of the variability in V02 max, R2 = .684. The estimated regression equation was VO2 = 2106.84 + .609*OUES. The positive slope for OUES 3 minutes was statistically significant, b = 0.609, t(48) = 10.194, p < .001, 95% CI [0.488, 0.729]. Final inspection of the residuals supported normality and homoscedasticity. 
Correlation

The Pearson correlation coefficient, r, is a standardised measure of the strength of the linear relationship between two variables. It can be calculated as follows:


A Pearson correlation can range from a perfect negative correlation, r = -1, to zero correlation, r = 0, and all the way through to a perfect positive correlation, r = 1. r and the slope, b, of a simple linear regression will have the same sign. The following plots provide examples of different strengths and types of correlations.


We can calculate a quick correlation in R using the cor() function:

> cor(VO2_Max,OUES_3,data = OUES)
[1] 0.8270676
But this does not give us a p-value or CI to test the null hypothesis.

We can perform a full correlation analysis between OUES and VO2 in R by installing the Hmisc library and using the rcorr() function.

> library(Hmisc)
> bivariate<-as.matrix(select(OUES, VO2_Max,OUES_3)) #Create a matrix of the variables to be correlated
> rcorr(bivariate, type = "pearson")
        VO2_Max OUES_3
VO2_Max    1.00   0.83
OUES_3     0.83   1.00

n= 50 


P
        VO2_Max OUES_3
VO2_Max          0    
OUES_3   0  
R reports the correlation between OUES and VO2 max to be r = .83 and the p-value = 0, which we write as p < .001. A hypothesis test for r has the following statistical hypotheses:

H0: r = 0 
HA: r ? 0
The p-value for r can be readily calculated by converting r to a t statistic:



Can you find this value in the output for the regression? That's correct, the t statistic was the same as the t statistic for the slope of the simple regression. Therefore, a two-tailed p-value for r can be calculated using the R formula:

> 2*pt(10.191,50-2,lower.tail=FALSE)
[1] 1.358369e-13
where df = n - 2 = 50 - 2 = 48. We find p < .001. Using the 0.05 level of significance, we must reject H0. There was a statistically significant positive correlation between OUES 3 minutes and VO2 max, r = .827, p < .001.

We can also test H0 using a confidence interval approach. We can use a normal approximation to calculate a confidence interval for r. First, r is converted to a z-score using the z-transformation:


This is easier in R...

> 0.5*(log((1+.827)/(1-.827)))
[1] 1.178569
This allows a 95% CI for r to be approximated using a normal distribution, z, as [z1, z2]:




To transform z1 and z2 back to a 95% CI for r, [?1, ?2]:




Therefore, r = 0.827, 95% CI [0.713, .899]. Alternatively, we can take away the hard work by using the CIr() function from the psychometric() package.

> install.packages("psychometric")
> library(psychometric) 
> CIr(r=cor(VO2_Max,OUES_3,data = OUES), n = 50, level = .95)
[1] 0.7128199 0.8985565
This confidence interval does not capture H0, therefore, H0 was rejected. There was a statistically significant positive correlation between OUES 3 minutes and VO2 max.

Example Write-up

A Pearson's correlation was calculated to measure the strength of the linear relationship between OUES 3 minutes and VO2 max. The positive correlation was statistically significant, r = .827, p < .001, 95% CI [0.713, .899].
Return to top

Recent Site Activity|Report Abuse|Print Page|Remove Access|Powered By Google Sites